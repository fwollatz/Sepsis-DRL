{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import gym\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(r\"../data/sepsis_final_data_withTimes.csv\")\n",
    "train_df.head()\n",
    "\n",
    "val_df = pd.read_csv(r\"../data/sepsis_final_data_withTimes.csv\")\n",
    "test_df = pd.read_csv(r\"../data/sepsis_final_data_withTimes.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-dd556f67a1f1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mclass\u001b[0m \u001b[0mDDDQN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mModel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDDDQN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "class DDDQN(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(DDDQN, self).__init__()\n",
    "        \n",
    "        \n",
    "        #two hidden layers of size 128,using batch normalization after each, Leaky-ReLU activation functions\n",
    "        self.d1 = tf.keras.layers.Dense(128, activation=None)\n",
    "        self.bn1 = tf.keras.layers.BatchNormalization()\n",
    "        self.lr1 = tf.keras.layers.LeakyReLU()\n",
    "        \n",
    "        self.d2 = tf.keras.layers.Dense(128, activation=None)\n",
    "        self.bn2 = tf.keras.layers.BatchNormalization()\n",
    "        self.lr2 = tf.keras.layers.LeakyReLU()\n",
    "        \n",
    "        # advantage and value streams\n",
    "        self.v = tf.keras.layers.Dense(1, activation=None)\n",
    "        self.a = tf.keras.layers.Dense(env.action_space.n, activation=None)\n",
    "\n",
    "    def call(self, input_data):\n",
    "        \n",
    "        x = self.d1(input_data)\n",
    "        x = self.bn1(x)\n",
    "        x = self.lr1(x)\n",
    "\n",
    "        x = self.d2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.lr2(x) \n",
    "        \n",
    "        v = self.v(x)\n",
    "        a = self.a(x)\n",
    "        Q = v +(a -tf.math.reduce_mean(a, axis=1, keepdims=True))\n",
    "        return Q\n",
    "\n",
    "    def advantage(self, state):\n",
    "        x = self.d1(state)\n",
    "        x = self.bn1(x)\n",
    "        x = self.lr1(x)\n",
    "\n",
    "        x = self.d2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.lr2(x) \n",
    "        \n",
    "        a = self.a(x)\n",
    "        return a\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class exp_replay():\n",
    "    def __init__(self, buffer_size= 1000000):\n",
    "        self.buffer_size = buffer_size\n",
    "        self.state_mem = np.zeros((self.buffer_size, *(env.observation_space.shape)), dtype=np.float32)\n",
    "        self.action_mem = np.zeros((self.buffer_size), dtype=np.int32)\n",
    "        self.reward_mem = np.zeros((self.buffer_size), dtype=np.float32)\n",
    "        self.next_state_mem = np.zeros((self.buffer_size, *(env.observation_space.shape)), dtype=np.float32)\n",
    "        self.done_mem = np.zeros((self.buffer_size), dtype=np.bool)\n",
    "        self.pointer = 0\n",
    "\n",
    "    def add_exp(self, state, action, reward, next_state, done):\n",
    "        idx  = self.pointer % self.buffer_size \n",
    "        self.state_mem[idx] = state\n",
    "        self.action_mem[idx] = action\n",
    "        self.reward_mem[idx] = reward\n",
    "        self.next_state_mem[idx] = next_state\n",
    "        self.done_mem[idx] = 1 - int(done)\n",
    "        self.pointer += 1\n",
    "\n",
    "    def sample_exp(self, batch_size= 64):\n",
    "        max_mem = min(self.pointer, self.buffer_size)\n",
    "        batch = np.random.choice(max_mem, batch_size, replace=False)\n",
    "        states = self.state_mem[batch]\n",
    "        actions = self.action_mem[batch]\n",
    "        rewards = self.reward_mem[batch]\n",
    "        next_states = self.next_state_mem[batch]\n",
    "        dones = self.done_mem[batch]\n",
    "        return states, actions, rewards, next_states, dones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class agent():\n",
    "      def __init__(self, gamma=0.99, replace=100, lr=0.001):\n",
    "          self.gamma = gamma\n",
    "          self.epsilon = 1.0\n",
    "          self.min_epsilon = 0.01\n",
    "          self.epsilon_decay = 1e-3\n",
    "          self.replace = replace\n",
    "          self.trainstep = 0\n",
    "          self.memory = exp_replay()\n",
    "          self.batch_size = 64\n",
    "          self.q_net = DDDQN()\n",
    "          self.target_net = DDDQN()\n",
    "          opt = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "          self.q_net.compile(loss='mse', optimizer=opt)\n",
    "          self.target_net.compile(loss='mse', optimizer=opt)\n",
    "\n",
    "\n",
    "      def act(self, state):\n",
    "          if np.random.rand() <= self.epsilon:\n",
    "              return np.random.choice([i for i in range(env.action_space.n)])\n",
    "\n",
    "          else:\n",
    "              actions = self.q_net.advantage(np.array([state]))\n",
    "              action = np.argmax(actions)\n",
    "              return action\n",
    "\n",
    "\n",
    "      \n",
    "      def update_mem(self, state, action, reward, next_state, done):\n",
    "          self.memory.add_exp(state, action, reward, next_state, done)\n",
    "\n",
    "\n",
    "      def update_target(self):\n",
    "          self.target_net.set_weights(self.q_net.get_weights())     \n",
    "\n",
    "      def update_epsilon(self):\n",
    "          self.epsilon = self.epsilon - self.epsilon_decay if self.epsilon > self.min_epsilon else self.min_epsilon\n",
    "          return self.epsilon\n",
    "\n",
    "          \n",
    "      def train(self):\n",
    "          if self.memory.pointer < self.batch_size:\n",
    "             return \n",
    "          \n",
    "          if self.trainstep % self.replace == 0:\n",
    "             self.update_target()\n",
    "          states, actions, rewards, next_states, dones = self.memory.sample_exp(self.batch_size)\n",
    "          target = self.q_net.predict(states)\n",
    "          next_state_val = self.target_net.predict(next_states)\n",
    "          max_action = np.argmax(self.q_net.predict(next_states), axis=1)\n",
    "          batch_index = np.arange(self.batch_size, dtype=np.int32)\n",
    "          q_target = np.copy(target)\n",
    "          q_target[batch_index, actions] = rewards + self.gamma * next_state_val[batch_index, max_action]*dones\n",
    "          self.q_net.train_on_batch(states, q_target)\n",
    "          self.update_epsilon()\n",
    "          self.trainstep += 1\n",
    "\n",
    "      def save_model(self):\n",
    "          self.q_net.save(\"model.h5\")\n",
    "          self.target_net.save(\"target_model.h5\")\n",
    "\n",
    "\n",
    "      def load_model(self):\n",
    "          self.q_net = load_model(\"model.h5\")\n",
    "          self.target_net = load_model(\"model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agentoo7 = agent()\n",
    "steps = 400\n",
    "for s in range(steps):\n",
    "  done = False\n",
    "  state = env.reset()\n",
    "  total_reward = 0\n",
    "  while not done:\n",
    "    #env.render()\n",
    "    action = agentoo7.act(state)\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    agentoo7.update_mem(state, action, reward, next_state, done)\n",
    "    agentoo7.train()\n",
    "    state = next_state\n",
    "    total_reward += reward\n",
    "    \n",
    "    if done:\n",
    "      print(\"total reward after {} episode is {} and epsilon is {}\".format(s, total_reward, agentoo7.epsilon))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
